---
title: "Penalized Regression"
author: "Nate Wells"
date: "October 11th, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	cache = F,
	fig.width = 6,
	fig.height = 4,
	warning = FALSE,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ISLR)
```

## Outline

In today's class, we will...

 
- Investigate the relationship between coefficient size and variance in linear models

- Discuss penalized regression models as means of improving MSE of linear models



# Penalized Regression

## Motivation

- Recall, for SLR, $\hat \beta_0, \hat \beta_1$ are given by

$$
\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} \qquad \hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$
  
  \pause

- Under the standard assumptions, the coefficients produced by least squares regression are unbiased.

\pause

- That is, if the true relationship between $Y$ and $X$ is linear $Y = \beta_0 + \beta_1 X + \epsilon$, then
$$
E[ \hat \beta_0] = \beta_0 \qquad E[\hat \beta_1 ] = \beta_1
$$
\pause
- Moreover, among all **unbiased** linear models, the least squares model has the lowest variance.

\pause

- Does this mean that the least squares model has the lowest MSE among all linear models?

  \pause
  
  - No! MSE is a combination of bias and variance. 
  
  - It is possible that a small *increase* in bias can correspond to large *decrease* in variance.
 
## Shrinking Coefficients

- Suppose the true relationship between $Y$ and $X_1, X_2$ is given by $$Y =1 + X_1 + 5 X_2 + \epsilon  \quad \epsilon \sim N(0,1).$$

- Let $\hat \beta_0, \hat \beta_1, \hat \beta_2$ be the model coefficient estimates given by least squares regression. Which of the following models has higher variance in predictor estimates? Higher bias?

  \pause

\begin{align*}
\textrm{Model 1:}   \quad   \hat y =& \hat \beta_0 + \hat \beta_1 x_1 + \hat \beta_2 x_2\\
\textrm{Model 2:}   \quad  \hat y =& \hat \beta_0 + 0.97\cdot \hat \beta_1  x_1 +  0.98 \cdot \hat \beta_2  x_2 
\end{align*}
  
\pause

- Model 2 has higher bias, but lower variance.

## A Linear Model

- Consider the following training data for the model: $Y = 1 + X_1 + 5 X_2 + \epsilon  \quad \epsilon \sim N(0,1)$

```{r cache = T}
set.seed(100)
x1 <- runif(20,0,1)
x2 <- 2 - 2*x1 + rnorm(20, 0, .3)
e<- rnorm(20,0,1)
y<- 1 + x1 + 5*x2 + e
sim_data <- data.frame(x1,x2,y)
```

```{r echo = F, eval = F}
summary(lm(y~x1+x2, data = sim_data))
cor(x1,x2)
```

```{r fig.height=3.5, cache = T}
ggplot(data = sim_data, aes(x = x1, y  = x2, color = y))+geom_point()+theme_bw()+labs(title = "20 training observations")
```
\pause

- What are some likely problems with the MLR model?

## Bias-Variance in Least Squares

- Using least squares, the model estimates are
$$
\hat Y = -0.5  + 2.8 X_1 +5.8 X_2
$$
\pause

- Let's consider variance and bias for estimate $Y$ when $X_1 = 0.25$ and $X_2 = .5$.

  \pause
  
  - Using the true model, the expected value of $Y$ is
$$
Y =1 + X_1 + 5 \cdot X_2 = 1 + 0.25 + 5 \cdot 0.5 = 3.75
$$

  \pause
  
  - Using the least squares model from training data, the predicted value of $Y$ is
$$
Y = -0.5  + 2.8 X_1 + 5.8 X_2 = -0.5  + 2.8 \cdot 0.25 + 5.8 \cdot 0.5 = 3.1
$$

\pause

- But how will the predicted value change if we repeat across 5000 simulations from the model?


## Simulation

\small

```{r echo = T, cache = T}
set.seed(1011)
test_point <- data.frame(x1 = 0.25, x2 = .5)

trials<-5000
prediction <- rep(NA, trials)
for (i in 1:trials){
  e<- rnorm(20,0,1)
  y<- 1 + x1 + 5*x2 + e
  sim_data <- data.frame(x1,x2,y)
  mod <- lm(y ~ x1 + x2, data = sim_data)
  prediction[i] <- predict(mod, test_point)
}

simulation <- data.frame(trial_num = 1:trials, prediction)
```


## Prediction Distribution

```{r fig.height = 3.5, cache = T}
simulation %>% 
  ggplot(aes(x = prediction))+geom_histogram( color = "white", fill = "steelblue")+theme_bw()+geom_vline(xintercept = 3.75, color = "red", size = 1)+geom_vline(xintercept = 3.78, color = "blue", size = 1)+labs(title = "Distribution of Predictions across 5000 simulations")+annotate(geom = "text", x = 2.5, y = 500, label = "True Value", color = "red")+annotate(geom = "text", x = 5.5, y = 500, label = "Average Prediction", color = "blue")
```
\pause

\small

```{r echo = T }
simulation %>% summarize(
  mean = mean(prediction), variance = var(prediction))
```

## A Shrunken Model

- Now suppose we use the model algorithm
$$
\hat y =  \hat \beta_0 + 0.97 \cdot \hat \beta_1 x_1 + 0.98\cdot \hat \beta_2  x_2 
$$

- Since $\hat \beta_0, \hat \beta_1, \hat \beta_2$ are unbiased, then the expected prediction for $Y$ when $X_1 = 0.25$ and $X_2 = 0.5$ is
$$
E[\hat y] = \beta_1 + 0.97 \cdot \beta_1 x_1 + 0.98 \cdot \beta_2 x_2 = 1 + 0.97  \cdot 0.25 +0.98 \cdot 5 \cdot 0.5 = 3.69
$$

\pause

- Based on the first simulation, the model estimate is
$$
\hat Y = -0.5  +  0.97 \cdot 2.8 X_1 + 0.98 \cdot 5.8 X_2    = -0.5 + 2.71 X_1 + 5.68 X_2
$$

\pause

- And the prediction when $X_1 = 0.25$ and $X_2 = 0.5$ is
$$
\hat y =  -0.5 + 2.71 X_1 + 5.68 X_2  =    -0.5 + 2.71  \cdot 0.25 + 5.68 \cdot 0.5 = 3.525
$$


## Simulation II

\small

```{r echo = T, cache = T}
set.seed(1001)

trials<-5000
prediction2 <- rep(NA, trials)
for (i in 1:trials){
  e<- rnorm(20,0,1)
  y<- 1 + x1 + 5*x2 + e
  sim_data <- data.frame(x1,x2,y)
  mod <- lm(y ~ x1 + x2, data = sim_data)
  b0 <- 1*coef(mod)[1]
  b1 <- .97*coef(mod)[2]
  b2 <- .98*coef(mod)[3]
  prediction2[i] <- b0 + b1*0.25 + b2*0.5
}

simulation2 <- data.frame(trial_num = 1:trials, prediction2)
```

## Prediction Distribution

```{r fig.height = 3.5, cache = T}
simulation2 %>% 
  ggplot(aes(x = prediction2))+geom_histogram( color = "white", fill = "steelblue")+
  theme_bw()+
  geom_vline(xintercept = 3.75, color = "red", size = 1)+
  geom_vline(xintercept = 3.65, color = "blue", size = 1)+
  labs(title = "Distribution of Predictions across 5000 simulations")+
  annotate(geom = "text", x = 1.5, y = 500, label = "Average Prediction", color = "blue")+
  annotate(geom = "text", x = 5.5, y = 500, label = "True Value", color = "red")
```
\pause

\small

```{r echo = T}
simulation2 %>% summarize(
  mean = mean(prediction2), variance = var(prediction2))
```

## Model Comparison

- True relationship: $Y = 1 + X_1 + 5 X_2 + \epsilon$

\pause

- Model 1: $\hat y =  \hat \beta_0 + \hat \beta_1 x_1 + \hat \beta_2 x_2$


```{r echo = F}
simulation %>% summarize(
  mean = mean(prediction), variance = var(prediction), avg_error = mean((prediction -3.75)^2))
```

\pause


- Model 2: $\hat y = \hat \beta_0 + 0.97\cdot \hat \beta_1  x_1 +  0.98 \cdot \hat \beta_2  x_2$

```{r echo = F}
simulation2 %>% summarize(
  mean = mean(prediction2), variance = var(prediction2),avg_error = mean((prediction2 -3.75)^2))
```

\pause

- It looks like the model with smaller coefficients actually performed better!

# Ridge Regression

## Shrinkage Penalty

- There are some situations in which multiple linear regression has high MSE:

  \pause
  
  - Predictors are strongly correlated (high variance)
  
  - Many predictors relative to data size (high variance)
  
  - Model form is non-linear (high bias)
  
\pause

- To improve models in the first two cases, we reduce MSE by reducing variance at the cost slight increase in bias.

\pause

- In the presence of multicollinearity or over-fitting, least squares estimates tend to be too large.

\pause

- To build a better model, we reduce the size of coefficients relative to least squares regression.


## Ridge Regression

- Recall that least squares regression estimates $\hat{\beta}_0, \hat{\beta}_1, \dots , \hat{\beta}_p$ for 
$$
\hat{y} = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \epsilon
$$
are obtained by finding the values of $\beta$ that minimize
$$
\mathrm{RSS} = \sum_{i = 1}^n (y_i - \hat{y}_i)^2 =\sum_{i = 1}^n \Big(y_i - \beta_0- \sum_{j = 1}^p \beta_j x_{ij}\Big)^2
$$

\pause

- To perform **Ridge Regression**, we instead find coefficients $\beta$ that minimize
$$
\textrm{RSS} + \lambda \sum_{i = 1}^p \beta_i^2 \qquad \textrm{where }  \lambda \geq 0 \textrm{ is tuning parameter}
$$

\pause

Why? 

\pause
  
- The term $\lambda \sum_{i = 1}^p \beta_i^2$ is the **shrinkage penalty**, and is small when the $\beta$ are small.

\pause

- With a shrinkage penalty, the algorithm prefers models with lower coefficients.

\pause

- This tends to reduce variance, at the cost of increased bias.


## Effects of the Tuning Parameter

- **Goal:** Find $\beta$ which minimize $\textrm{RSS} + \lambda \sum_{i = 1}^p \beta_i^2$

\pause

- What will happen to $\beta_i$ as $\lambda \to \infty$? As $\lambda \to 0$?

\pause

- What will happen to $\beta_0$ as $\lambda \to \infty$? As $\lambda \to 0$?

\pause

- What happens to MSE as $\lambda \to 0$ or $\lambda \to \infty$?

\pause

```{r}
L <-  seq(from  = -2, to = 10, by = 0.5)
B<- 1.5*pnorm(L, 3, 2)
V<- 1.4*(1 - pnorm(L, 2, 2))+.2
M <- B+V+.5
d<-data.frame(L,B,V,M)
```

```{r fig.height=3, cache = T}
ggplot(d, aes(x = L))+geom_line(aes(y = B), color = "red")+geom_line(aes(y = V), color = "blue")+geom_line(aes(y = M), color = "purple")+ geom_hline(yintercept = .5,linetype ="dashed")+ labs(x = "Lambda", y = "Error")+theme_bw()+annotate(geom = "text", x = 7.5, y = 1.3, label = "Bias", color = "red")+
  annotate(geom = "text", x = 2, y = 1.75, label = "MSE", color = "purple")+annotate(geom = "text", x = 7.5, y = 0.35, label = "Variance", color = "blue") +annotate(geom = "text", x = 0, y = 0.6, label = "Irreducible Error", color = "black")+
  scale_x_continuous(labels = c("10^-2.5", "10^0", "10^2.5", "10^5","10^7.5", "10^10"))+
  labs(title = "Bias Variance Tradeoff with Shrinkage Penalty")
```

## Simulation

- Consider a linear model with 9 predictors and 100 observations.
$$ y = 10 +   1 x_1 + 2x_2 \dots + 8  x_{8} + 9x_{9} + \epsilon \quad \epsilon \sim N(0,4)$$
```{r cache = T}
n_pred <- 9
n_obs <- 100
set.seed(10)
sim_data2 <- data.frame(x = runif(n_pred*n_obs), predictor = rep(1:n_pred, each = n_obs), obs = rep(1:n_obs, times = n_pred))%>% pivot_wider(names_from = predictor, values_from = x) %>% select(-obs)

y<- rep(NA, n_obs)
for(i in 1:n_obs){
  y[i] <- 1 + sum(as.numeric(sim_data2[i,1:9])*1:9) + rnorm(1,0,2)
}

sim_data2 <- cbind(sim_data2, y)
```

```{r cache = T}
library(glmnet)
y <- sim_data2$y
x <- model.matrix(y ~., data = sim_data2)[,-1]
my_net <- glmnet(x,y)
```

\pause

\tiny

```{r}
summary(lm(y ~ ., sim_data2))
```


## Simulation

- What happens to the size of coefficients as $\lambda$ gets larger?

```{r cache = T}
library(broom)
tidied <- tidy(my_net) %>% filter(term != "(Intercept)")
ggplot(tidied, aes(lambda, estimate, group = term, color = term)) +
    geom_line(size = 1) +
    scale_x_log10()+
  theme_bw()+
  labs(title = "Coefficent estimates as function of penalty")
```
## Effect of Scale

- Suppose $\hat{y} = 1 + 0.01 x_1 + 20 x_2$ is the best fitting linear model for $Y$ using $X_1$ and $X_2$, and that both are statistically significant.
  
  \pause
  
  - Are we justified in saying that $X_2$ is a more important predictor than $X_1$?
  
  \pause

  - What if $\mathrm{sd}(x_1) = 10000$ and $\mathrm{sd}(x_2) = .1$? 
  
- Suppose we first standardize $X_1$ and $X_2$ by subtracting off their means and dividing by their standard deviations:
  $$
  Z_1 = \frac{X_1 - \mu_1}{\sigma_1} \qquad Z_2 = \frac{X_2 - \mu_2}{\sigma_2}
  $$
  
\pause
  
- If we build a model and find $\hat{y} = 1 + 0.01 z_1 + 20 z_2$, where $Z_1$ and $Z_2$ are standardized, are we now justified in saying that $Z_2$ is more important than $Z_1$?
  
  \pause
  
  - Assuming both are statistically significant, we are probably justified.
  
  
  
## Scale

- The coefficients in the least squares regression equation are **scale-equivalent**

  \pause
  
  - That is, scaling a predictor by a value $c$ just leads to scaling the estimate by $1/c$. 
  
  \pause
  
  - The predicted value is the same, regardless of scale.
  
  \pause
  
  - Therefore, rescaling predictors *does not* change the fit of the model (RSS is the same)
  
  \pause

  - Suppose $y = 1 + 0.01 x_1 + 20 x_2$, $\sigma_1 = 10000, \sigma_2 = 0.1$, and both $x_1, x_2$ have mean $0$.
  
  - After rescaling, $z_1 = \frac{x_1}{10000}, z_2 = \frac{x_2}{0.1}$ and the linear model is
$$
y = 100z_1 + 2z_2
$$

- However, for Ridge Regression, coefficient estimates can change depending on scale.

  \pause
  
  - Recall the shrinkage penalty is $\lambda \sum_{i =1}^2 \beta_i^2 = \lambda(0.01^2 + 20^2)$
  
  \pause
  
  - Which models will ridge regression favor?
  
\pause

- Ridge regression is most effective if predictors are standardized first. 