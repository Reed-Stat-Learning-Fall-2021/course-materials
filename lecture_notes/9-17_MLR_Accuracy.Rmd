---
title: "MLR: Accuracy and Extensions"
author: "Nate Wells"
date: "September 14th, 2020"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", message = F, cache = T)
library(tidyverse)
library(knitr)
library(ggthemes)
library(moderndive)
library(ISLR)
```

## Outline

In today's class, we will...



- Quantify model accuracy for linear regression models (both simple and multiple)

- Generalize to include categorical variables and non-linear terms
 

 
# Assessing Model Accuracy

## How Strong is a Linear Model?

In an linear model model, $$Y = f(X)+ \epsilon $$ So even if we could perfectly predict $f$ using $\hat{f}$, our model would still have non-zero MSE. 

\pause
 
The **Residual Standard Error** (RSE) measures the average size of deviations of the response from the linear regression line, is given by

$$\mathrm{RSE} = \sqrt{\frac{1}{n- 1 - p}\mathrm{RSS} } = \sqrt{\frac{1}{n- 1 - p} \sum_{i =1}^n (y_i - \hat{y}_i)^2} $$

\pause

It has the property that

$$E(\mathrm{RSE}^2 ) \approx \mathrm{Var}(\epsilon)$$

## Poll 1

Which of the following are most likely to decrease as more and more predictors are added to a linear model (select all that apply)?

(a) test MSE
(b) training MSE
(c) RSS
(d) RSE
(e) $\mathrm{Var}(\epsilon)$
 
## The $R^2$ statistic

Large RSE indicates poor model fit, while small RSE indicates good fit. But how do we determine how small is **small**?

  \pause
  
  - The answer depends on the units of $Y$
  
\pause

An alternative, standardized measure of goodness of fit is the $R^2$ statistic:
$$
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \qquad \textrm{ where } \mathrm{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2
$$

\pause

- The value of $R^2$ is always between $0$ and $1$, and represents the percentage of variability in values of the response just due to variability in the predictors.

## Values of R^2

If $R^2 \approx 1$: nearly all the variability in response is due to variability in the predictor variable.
  
  \pause
  
  
```{r echo = F, out.width="55%"}

set.seed(254)
X<-runif(20, 1, 4)
Y<-X + rnorm(20,0,0.2)

dd<-tibble(X,Y)

ggplot(dd )+
  geom_point( aes(x = X, y =Y))+
  geom_point( aes( x = 0, y = Y), color = "darkgrey")+
  geom_point( aes( x = X, y = 0),color = "darkgrey")+
  annotate(geom = "text", x = 1, y = 3, label = "R = 0.97")+
  annotate(geom = "text", x = 1, y = 2.5, label = "R^2 = 0.94")
```


## Values of $R^2$

If $R^2 \approx 0$: almost none of the variability in response is due to variability in the predictor variable.
  
  \pause
  
  
```{r echo = F, out.width="55%"}

set.seed(254)
X<-runif(20, 1, 4)
Y<- X/10+rnorm(20,0,1)

dd<-tibble(X,Y)

ggplot(dd )+
  geom_point( aes(x = X, y =Y))+
  geom_point( aes( x = 0, y = Y), color = "darkgrey")+
  geom_point( aes( x = X, y = 0),color = "darkgrey")+
  annotate(geom = "text", x = 1, y = 3, label = "R = 0.27")+
  annotate(geom = "text", x = 1, y = 2.5, label = "R^2 = 0.07")
```

## Formulas for $R^2$ in terms of correlation

For SLR, 
$$
R^2 = \left[ \mathrm{Cor}(X,Y)\right]^2 = \left[ \frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y) }} \right]^2= \left[ \frac{  \sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})    }{\sqrt{ \sum_{i=1}^n(x_i - \bar{x})^2 } \sqrt{ \sum_{i=1}^n(y_i - \bar{y})^2 }} \right]^2
$$

\pause

For MLR,
$$
R^2 = \left[ \mathrm{Cor}(Y, \hat{Y}) \right]^2
$$

\pause

We will usually use software to compute $R^2$.

## Model Accuracy in `R`

\fontsize{7pt}{7.2}\selectfont
```{r echo = T}
mod_credit<-lm(Balance ~ Income + Limit , data = Credit)

summary(mod_credit)
```
\fontsize{9pt}{7.2}\selectfont

\pause

We can use `summary(mod)$r.sq` or `summary(mod)$sigma` to access $R^2$ and $\mathrm{RSE}$ directly.

## Adjusted $R^2$

- It turns out that the samples's $R^2$ gives a **biased** estimate of the variability in the *population* explained by the model.

\pause

- Instead, we use the adjusted R:
\fontsize{7pt}{7.2}\selectfont
$$R^2_{\mathrm{adjusted}} =   1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \frac{n-1}{n - p -1}$$

\pause

- This adjusted $R^2$ is usually a bit smaller than $R^2$, and the difference decreases as $n$ gets large.

## Testing Significance

Suppose we wish to test whether at least one predictor has a significant linear relationship with the response.

\pause

Why would it be incorrect to conduct $p$ many significant tests comparing each predictor to the response?


## The Hypothesis Test
Goal: test whether any predictors are significant.

\pause
Hypotheses:
$$H_0: \beta_ 1 = \dots = \beta_p = 0 \qquad H_a: \textrm{ at least one of } \beta_i \neq 0$$

\pause

Test statistic:
$$
F = \frac{(\mathrm{TSS} - \mathrm{RSS})/p}{\mathrm{RSS}/(n-p-1)}  
$$

\pause

Under the null hypothesis, $F$ is approximately $F$-distributed with $p,n-p-1$ parameters.

\pause

```{r fig.height=2, fig.width=6, out.width = "70%"}
X = seq(0,8, by = .1)
Y= df(X, 4, 20)
data.frame(X,Y) %>% 
  ggplot(aes(x= X, y = Y))+
  geom_line()+
  labs(title = "Density for 4 predictors, 25 observations", x = "F")
```

## Typical Values of the $F$ statistic

Provided conditions for linear regression are met, 
$$
E \left[ \frac{\mathrm{RSS}}{n - p - 1} \right] = \sigma^2 = \mathrm{Var}(\epsilon)
$$

\pause

And if $H_0$ is also true, then
$$
E \left[ \frac{\mathrm{TSS} - \mathrm{RSS}}{p} \right] = \sigma^2 = \mathrm{Var}(\epsilon)
$$

\pause

Hence, if there is truly no relationship between any of the predictors and the response, then
on average,
$$
F = \frac{(\mathrm{TSS} - \mathrm{RSS})/p}{\mathrm{RSS}/(n-p-1)} = 1
$$

\pause

Moreover, it is unlikely that $F$ is drastically larger than $1$.

## Poll 2: TSS and RSS

Suppose we have a linear model with $25$ observations and $4$ predictors. Which of the following provides the best evidence of a relationship between the response and at least 1 of the predictors?

(a) $\mathrm{TSS} = 64$, $\mathrm{RSS} = 4$
(b) $\mathrm{TSS} = 4$, $\mathrm{RSS} = 16$
(c) $\mathrm{TSS} = 48$, $\mathrm{RSS} = 8$
(d) $\mathrm{TSS} = 4$, $\mathrm{RSS} = 4$

## Improving Model Accuracy

What do we do when model accuracy is low (either high $\mathrm{RSE}$ or low $R^2$)?

\pause

- If some variables are strongly correlated, remove some redundant ones.

  \pause
  
  - This process is known as *backwards elimination*. 
  
  - Start with the full model, remove the variable with highest $p$-value, and refit. Continue to do so until accuracy ceases to improve.
  
\pause

- If $\epsilon$ is too large, add further variables.

  \pause
  
  - This process is known as *forward selection*. 
  
  - Start with the null model, create $p$ many SLR models (one for each predictor), and select the one with best accuracy. Repeat with this new model, creating $p-1$ two predictor models (one for each remaining predictor). Continue until accuracy ceases to improve.
  
\pause

- Is it possible that none of these models will have the best possible accuracy among all subsets of predictors?

  \pause
  
  - Yes. But we'll cover detailed model selection in Chapter 6.
  
# Extending the Linear Model

## Qualitative Predictors

Thus far, we have assumed all predictors are quantitative (taking values on a scale).

\pause

- It would nice to include qualitative or categorical predictors in our model.

\pause

- But if we try to include them naively, we immediately run into trouble:

\pause

$$
\hat{\mathrm{Debt}} = f(X_1, X_2, X_3) = \hat{\beta}_0 + \hat{\beta}_1 \cdot \mathrm{Income} + \hat{\beta}_2 \cdot \mathrm{Limit}  + \hat{\beta}_3 \cdot \mathrm{Gender}
$$

\pause
\small \phantom{Suppose suppose } Suppose $\hat{\beta}^T = \begin{pmatrix} -400 & -7.5 & .25 & 2.5\end{pmatrix}$ \normalsize
$$
\hat{\mathrm{Debt}} = f(10, 4000, \textrm{Female}) = -400 -7.5 \cdot 10 +.25\cdot 4000 + 2.5 \cdot \textrm{Female} = ???
$$

## Coding and Dummy Variables

- For binary categorical variables, we create a new *quantitative* variable by coding the first level as $0$ and the second as $1$. 

  \pause
  
  - For `Gender', we could code: $1 \leftarrow \textrm{Female} \quad 0 \leftarrow \textrm{Male}$
  
  \pause
  
$$
\hat{\mathrm{Debt}} = f(7.5, 4000, \textrm{Female}) = -400 -7.5 \cdot 10 + 0.25\cdot 4000 + 2.5 \cdot 1 = 527.5
$$

\pause

- In general, if $X_1$ is quantitative and $X_2$ is categorical, the resulting model will be
$$
\hat{Y} = f(X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 = \begin{cases} ( \beta_0 + \beta_2) + \beta_1 X_1,& \textrm{ if obs. in 1st level},\\ 
  \beta_0 + \beta_1 X_1,& \textrm{ if obs. in 2nd level}.\\ \end{cases}
$$

\pause

Note that both regression lines have the same slope, but different intercept.

## Scatterplot

```{r}
X1<-runif(50,0,1)
Y1<-2+2*X1+rnorm(50,0,.25)
X2<-runif(50,0,1)
Y2<-3+1*X2+rnorm(50,0,.25)

my_data<-data.frame(X=c(X1,X2), Y=c(Y1,Y2), G = rep(c(0,1), each = 50)) %>% mutate(G= as.factor(G))
```

```{r}
my_data1<-filter(my_data, G=="0")
my_data2<-filter(my_data, G=="1")

mod_2<-lm(data = my_data, Y ~ X + G)
coeffs<-(summary(mod_2))$coefficients


```

```{r fig.height=4, fig.width=6, out.width="70%"}
ggplot( )+geom_point(data = my_data, aes(x = X, y = Y, color = G)) +
  geom_abline(slope = coeffs[2], intercept =coeffs[1] + coeffs[3], color = 4 ) +
  geom_abline(slope = coeffs[2], intercept =coeffs[1], col = 2  )
```

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X + \hat{\beta}_2 G =2.28 + 1.41 X + 0.53 G
$$

## The model in `R`

\small 
```{r echo = T}
mod_2<- lm(data = my_data, Y ~ X + G)
summary(mod_2)
```

## Poll 3: MLR Slope Interpretation

The slope on a (binary) categorical variable $G$ tells us (select all that apply)

(a) How much we expect the response to change if we increase the value of $G$ from $0$ to $1$, while holding all else constant.


(b) The difference in the average response between observations in the two categories.


(c) The value of the response variable if $G$ equals 0. 


(d) The distance between the two regression lines on the 2d scatterplot

## Categorical Variables with more than 2 levels.

We extend to variables with more than 2 levels by creating binary variables for each level.

\pause

In the `Credit` data set, the `Ethnicity` variable takes 3 levels: `African American`, `Asian`, `Caucasion`. (As with `Gender`, the levels here are incomplete)

\pause

For categorical variable $X_i$ with levels $j = 1, \dots, k$, create a dummy variables $x_{ij}$ by
\small
$$
x_{ij} = \begin{cases}1,& \textrm{obs. in level $j$},\\ 0,& \textrm{obs. not in level $j$}, \end{cases}
$$

\normalsize
\pause

For example,
\small
$$
\textrm{Eth}_{AA} = \begin{cases}1,& \textrm{obs. is African American},\\ 0,& \textrm{obs. is not African America} \end{cases}
$$
$$
\textrm{Eth}_{A} = \begin{cases}1,& \textrm{obs. is Asian},\\ 0,& \textrm{obs. is not Asian} \end{cases}
$$
$$
\textrm{Eth}_{C} = \begin{cases}1,& \textrm{obs. is Caucasion},\\ 0,& \textrm{obs. is not Caucasion} \end{cases}
$$

  \pause
  
  - Every observation evaluates to 1 in exactly 1 dummy variable.
  
## Categorical Variables in `R`

\small

```{r echo = T}
credit_mod <- lm(Balance ~ Limit + Income + Gender + Ethnicity, data = Credit)
summary(credit_mod)$coefficients
```

$$
\hat{\textrm{Balance}} = -395.7 + 0.26 \cdot \textrm{L} - 7.67 \cdot \textrm{I} + 1.91 \cdot \textrm{G}_F + 26.88 \cdot \textrm{E}_A + 3.76 \cdot \textrm{E}_C
$$