---
title: "Principal Component Analysis"
author: "Nate Wells"
date: "December 5th, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	cache = F,
	fig.width = 6,
	fig.height = 4,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
```



## Outline

In today's class, we will...

- Discuss Principal Component Analysis as an example of unsupervised learning

- Implement PCA in R and interpret PCA in context



# Principal Component Analysis 

## Unsupervised Learning

Thus far, we have concerned ourselves with **supervised learning** methods, where we predict the value of a response $Y$ based on the values of the predictors $X_1, \dots , X_n$.

  \pause
  
  - Ex: Predicting the price of a house in Ames, Iowa
  
\pause

In **unsupervised learning**, we use statistical tools to analyze relationships among several features $X_1, \dots, X_n$ without an associated response variable $Y$.

  \pause
  
  - Ex: Finding subgroups of similar Beatles songs.

\pause

- Compared to supervised learning, analysis of unsupervised learning methods tend to be more subjective (since we can't assess accuracy using a response variable)

\pause

- But unsupervised learning represents an instrumental part of exploratory data analysis (and of pattern recognition, more generally)

## PCA

To compute the principal components $Z_1, Z_2, \dots , Z_p$ on a data set with variables $X_1, \dots, X_p$, we do **not** ever use the values of a response variable $Y$.

  \pause 
  
  - Although for Principal Component Regression, we did later use those principal components to make predictions about $Y$.
  
\pause 

PCA can be used as a means of unsupervised learning and exploratory data analysis.

  \pause
  
  - PCA finds the consecutive linear combinations of predictors (or features) that have the most variance, once prior linear combinations are taken into account.
  
## PCA

The first principal component of $X_1, \dots , X_p$ is the normalized linear combination 
$$
Z_1 = \phi_{11}X_1 + \dots + \phi_{p1}X_p \qquad \textrm{ with }\sum \phi_{i1}^2 = 1
$$
  \pause
  
  - The vector $\phi_1 = \begin{pmatrix} \phi_{11} \\ \vdots \\ \phi_{p1} \end{pmatrix}$ is called the loading (and entries $\phi_{i1}$ the loading of $X_i$)

\pause

- The first principal component loading vector solves the optimization problem:

$$
\phi_{1} = \textrm{argmax}_{\phi_{11}, \dots , \phi_{p1}}\left\{ \frac{1}{n} \sum_{i = 1}^n \left( \sum_{j =1}^p \phi_{j1}x_{ij} \right)^2 \right\} \textrm{ given } \sum_{j =1}^p \phi_{ji}^2 = 1
$$
  
\pause

- The vector of loadings $\phi_1 \in \mathbb R$ points in the direction in feature space along which the data varies the most.

## PCA

The second principal component $Z_2$ is the linear combination of $X_1, \dots , X_p$ that has maximal variance among all lin. combos. that are uncorrelated with $Z_1$, and takes the form
$$
Z_2 = \phi_{12}X_1 + \dots + \phi_{p2}X_p \qquad \textrm{ with }\sum \phi_{i1}^2 = 1 \textrm{ and } \mathrm{Corr}(Z_1,Z_2)=0
$$

\pause

- $Z_2$ can also be obtained by projecting all observations onto the hyperplane perpendicular to $\phi_{1}$ and finding the 1st principal component of the resulting data set.

\pause

In general, the $k$th principal component is a linear combination that has maximal variance among all combos that are uncorrelated with $Z_1, \dots, Z_{k-1}$
$$
Z_k = \phi_{1k}X_1 + \dots + \phi_{pk}X_p \qquad \textrm{ with }\sum \phi_{i1}^2 = 1 \textrm{ and } \mathrm{Corr}(Z_j,Z_2)=0, \, 1 \leq j \leq k-1
$$

## Alternate Geometric Perspective

**Perspective 1**: Principal components are directions in feature space along which data vary the most.

\pause

**Perspective 2**: The first $M$ principal components are the best $M$-dimensional approximation to the $p$-dimensional data set.

\pause

- Observe that the loading vector $\phi_1$ is the line in $p$-dim space that is *closest* to the $n$ observations in the data set. 
\pause

- Together, the loading vectors $\phi_1, \phi_2$ generate a plane in $p$-dim space that is closest to the $n$ observations

\pause

- Generally, the first $M$ loading vectors $\phi_1, \dots, \phi_p$ generate an $M$-dimensional hyperplane in $p$-dim space that is closest to the $n$ observations.

\pause

$$
x_{ij} \approx \sum_{m =1}^M z_{im} \phi_{jm} \qquad \textrm{ where } z_{im} = \phi_{1m}x_{im} + \dots + \phi_{pm}x_{ip}
$$

## Visual
Reduction from $p =3$ to $p =2$ via principal components.

```{r out.width="90%"}
include_graphics("img/pca-3-2.png")
```

\pause

How does this differ from least squares regression?

## Properties of PCA

How much information is lost when we project the data set onto the hyperplane spanned by the first $M$ principal component loading vectors?

\pause

- The *Total Variance* (TV) of the data set is
$$
\textrm{TV} = \sum_{j = 1}^p \mathrm{Var}(X_j) = \sum_{j = 1}^p \frac{1}{n} \sum_{i =1}^n x_{ij}^2
$$

\pause

- While the variance explained by the $m$th principal component $V_m$ is
$$
V_m = \frac{1}{n} \sum_{i =1}^n z_{im}^2 = \frac{1}{n}\sum_{i = 1}^n \left( \sum_{j = 1}^p \phi_{jm}x_{ij} \right)^2
$$

\pause

- Thus, the *Proportion of Variance Explained* by the $m$th principal component $\textrm{PVE}_m$ is
$$
\mathrm{PVE}_m = \frac{V_m}{TV} = \frac{\sum_{i = 1}^n \left( \sum_{j = 1}^p \phi_{jm}x_{ij} \right)^2}{ \sum_{j = 1}^p   \sum_{i =1}^n x_{ij}^2}
$$

## How many principal components?

In Principal Component Regression, we can use CV to decide how many principal components should be used in a model.

  \pause
  
  - But in unsupervised exploratory analysis, CV is not available (why?)
  
\pause

Instead, we can create the *scree plot* of $\textrm{PVE}_m$ versus $m$ and look for the point of diminishing returns (called the *elbow*)

\pause

```{r fig.height=3, fig.width=8}
set.seed(1)
x<-seq(from = 1, to = 10, by = 1)
e<-rnorm(10, -x/100, x/100)
e[1]<-0
e[2]<-0
e[3]<-0
V <- 45/x^2 + e
d<-data.frame(V, t = seq(from = 1, to = 10, by = 1))
ggplot(d, aes( x = t, y = V))+geom_line()+theme_bw()+labs(title = "Scree Plot", x = "components", y = "PVE")
```

\pause

An alternative is to investigate the data structure present in the first several principal components, and then continue adding further components until the structures of interest no longer change substantially

# PCA in R

## PCA Example

12 perfumers were asked to rate 12 perfumes on 11 scent adjectives

```{r echo = FALSE}
experts <- read.table(file = "data/perfumes.csv", header = TRUE, sep = ",",
                      dec = ".", quote= "\"")

experts <- experts %>%
  filter(Session == 1) %>%
  select(-Panelist, -Session, -Rank, -Wrapping) %>%
  group_by(Product) %>%
  summarize(spicy = mean(Spicy),
            heady = mean(Heady),
            fruity = mean(Fruity),
            green = mean(Green),
            vanilla = mean(Vanilla),
            floral = mean(Floral),
            woody = mean(Woody),
            citrus = mean(Citrus),
            marine = mean(Marine),
            greedy = mean(Greedy),
            oriental = mean(Oriental))
names(experts)[1] <- "perfume"
 options(scipen=1, digits=2)
```

\small
```{r}
 names(experts)[-1]
```
\normalsize

\pause

Each was rated on a scale of 1-10, and ratings for each perfume were averaged across experts.

\small
```{r echo = T}
head(experts)
```

## Fitting the PCA

To fit a pca model, we use the `prcomp` function in `Base` R.
\small
```{r echo = T}
pca1 <- prcomp(experts[, -1], scale = TRUE)
```
\normalsize

\pause

The output of `prcomp` contains a number of useful quantities

\small

```{r echo  = T}
names(pca1)
```

\normalsize

\pause

The rotation value contains the principal component loadings

\tiny
```{r echo=T}
 kable(pca1$rotation)
```



## Visualize

How can we visualize in R?

\pause

- Representing the data set itself requires 11 dimesions.

\pause

- Representing all pairwise structure requires $\binom{55}{2} = 55$ pairwise scatterplots

\pause

We can use principal components to focus our attention on small dimensional representation which describes most of the structure.

## Scatterplot

```{r    out.width = "70%"}
 

library(ggplot2)
library(ggrepel)
d <- as.data.frame(pca1$x)
d$perfume <- experts$perfume
levels(d$perfume)[levels(d$perfume)=="Cin\xe9ma"] <- "Cinema"
p1 <- ggplot(d, aes(x = PC1, y = PC2)) +
  geom_point(size = 3) +
  geom_text_repel(aes(label = perfume)) +
  xlim(c(-5, 5)) +
  theme_bw(base_size = 18)+
  labs(x ="Z1", y = "Z2")
p1
```

## Interpretation

Effectively interpreting principal the loading vector for principal components usually requires domain knowledge. But we can try!

\pause


What does $Z_1$ represent? (i.e for what values of $x$ is $Z_1$ large? small?)

\small

```{r}
pca1$rotation[, 1]
```
\normalsize

\pause
\vspace{2 em}

What does $Z_2$ represent?

\small

```{r}
pca1$rotation[, 2]
```

## Another Visualization

\small
```{r fig.align="center", fig.height = 6, fig.width=6, echo = T, out.width="55%"}
biplot(pca1)
```

## Scree Plot

\small

```{r echo = T, fig.align="center", fig.height = 5 , fig.width=7, out.width = "65%"}
d <- data.frame(PC = 1:11, PVE = pca1$sdev^2 / sum(pca1$sdev^2))

ggplot(d, aes(x = PC, y = PVE)) + geom_line() + geom_point() +
  theme_bw(base_size = 18)
```