---
title: "Principal Component Regression"
author: "Nate Wells"
date: "December 3rd, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	cache = F,
	fig.width = 6,
	fig.height = 4,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
```

## Outline

In today's class, we will...

- Discuss Principal Component Analysis as a means of dimensionality reduction for regression

- Implement PCR in R


# Principal Component Regression

## Dimensionality Reduction

Suppose you collect a sample of $n$ observations on $p$ predictors $X_1, \dots, X_p$, where $p$ is relatively large. Suppose further that some of the predictors are correlated with one another. 

\pause
 
- Any predictive model for a response $Y$ based on all of the correlated variables will underperform due to instability in parameter estimates.

\pause

It may be difficult to fit complex models accurately, given limited number of observatiosn compared to predictors.

\pause

- If $p$ is larger than $n$, it may not be possible to fit certain models to the data (for example MLR models cannot be used)

\pause

One solution is to perform variable selection and drop some less useful predictors.

\pause

- But dropping variables completely loses possible valuable information.

\pause

- Instead, we can combine variables into new ones that adequately describe the variance in the data, and drop those that have limited utility in explaining that variance.

## PCA Overview

Consider the weight and belly circumference for a random sample of 100 toddlers. 

```{r out.width = "45%"}
set.seed(1)
n=100
x<-rnorm(n,24,1)
y<-rnorm(n,-1,1)
Weight<-x
Belly<-.75*x+0.75*y+2.5

d<-data.frame(Weight,Belly)
ggplot(d, aes(x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x = "Weight", y = "Belly", title = "Weight and Belly Circumference for 100 Toddlers")
```

\pause
\normalsize
What are the approximate standard deviations of `Weight` and `Belly`?

\pause

\small

```{r}
d %>% summarize(sd_Weight = sd(Weight), sd_Belly = sd(Belly))
```

 




## PCA Overview

Consider the weight and belly circumference for a random sample of 100 toddlers. 

```{r echo=F, fig.height=4, fig.width=6, out.width="50%"}
set.seed(1)
ggplot(d, aes(x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x = "Weight", y = "Belly", title = "Weight and Belly Circumference for 100 Toddlers")
```

 

\normalsize
 
But how much of the variation in `Belly` circumference is just due to variation in `Weight`?

\small
\pause

```{r}
R_sq<-(cor(Weight,Belly))^2
data.frame(R_sq)
```



## PCA Overview

Can we find a line along which the observations vary the most?

```{r}
Weight_mean <- mean(Weight)
Belly_mean <- mean(Belly)

pca<-prcomp(d)
phi1<-pca$rotation[1,1]
phi2<-pca$rotation[2,1]
b1 <- phi1/phi2
b0 <-  Belly_mean - b1*Weight_mean
```

\pause

```{r out.width = "45%"}
ggplot(d, aes( x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x = "Weight", y = "Belly") +coord_fixed()+geom_abline(intercept = b0, slope = b1, color = "blue", size = 1) 
```

## PCA Overview 

How much variation occurs perpendicular to this line?

```{r out.width = "45%"}
ggplot(d, aes( x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x =  "Weight", y = "Belly")+geom_abline(intercept = b0, slope = b1, color = "blue", size = 1)+geom_abline(slope = -1/b1, intercept = Belly_mean + 1/b1*Weight_mean, color = "red", linetype = "dashed", size = 1) + coord_fixed()
```


## PCA Definition

The *first principal component* $Z_1$ is the direction along which there is the greatest variability in the data. 

  \pause
  
  - That is, if we project the observations onto this line, the resulting projected observations would have the greatest possible variance.

  \pause
  
  - Projecting a point onto a line amounts to finding the location on the line closest to the given point.
  
\pause

We can express the first principal component as a linear combination of the centered predictors $X_i - \bar{X_i}$, where $\phi_{i1} \in \mathrm{R}$ and $\phi_{11}^2+ \dots + \phi_{p1}^2 = 1$:
  
\pause

$$
Z_1 = \phi_{11} (X_1 - \bar{X}_1) + \phi_{21}  (X_2 - \bar{X}_2) + \dots + \phi_{p1} (X_p - \bar{X}_p) 
$$

\pause

- Alternatively, we could express $Z_1$ as an affine linear combination of the predictors themselves (affine meaning including a constant term)

## PCA Example

The first principal component





```{r out.width = "45%"}
ggplot(d, aes( x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x =  "Weight", y = "Belly")+geom_abline(intercept = b0, slope = b1, color = "blue", size = 1) + coord_fixed()+geom_point(x = Weight_mean, y = Belly_mean, color = "red", size = 2)

```


$$
Z_1 = 0.66 \cdot(\textrm{Weight} - 24.1 )  + 0.75\cdot (\textrm{Belly} - 19.8)
$$

## PCA Example

What is leftover?

```{r}
a<- -b1
b<- 1
c<- -b0

get_distance <- function(x0,y0, a,b,c){
  (a*x0+b*y0+c)/sqrt(a^2 + b^2)
}

get_x <- function(x0,y0, a,b,c){
  (b*(b*x0-a*y0)-a*c)/(a^2 +b^2)
}

get_y <- function(x0,y0, a,b,c){
  (a*(-b*x0+a*y0)-b*c)/(a^2 +b^2)
}
```

```{r out.width = "75%"}
d <- d %>% mutate(z1_x = get_x(Weight, Belly,-b1, 1, -b0), z1_y = get_y(Weight,Belly,-b1, 1, -b0), pos = sqrt( (z1_x - Weight_mean)^2 + sqrt(z1_y - Belly_mean)^2), distance = get_distance(Weight, Belly, -b1, 1, -b0))

ggplot(d)+geom_point(aes(x=Weight, y = Belly))+
  geom_point(aes(x=z1_x, y = z1_y), color = "blue")+
  geom_abline(intercept = b0, slope = b1)+theme_bw()+geom_segment(aes(x = Weight, y = Belly,xend = z1_x, yend = z1_y), linetype = "dashed")+coord_fixed()
 

```

## PCA Example

What is leftover?


```{r out.width = "75%"}
ggplot(d, aes( x = pos, y = distance))+geom_point()+theme_bw()+labs(x = "1st Principal Component", y = "Leftovers")+geom_abline(slope = 0, intercept = 0, color = "blue") + coord_fixed()+geom_point(aes(x = pos, y = 0), color = "blue")+geom_segment(aes(x = pos, xend = pos, y = distance, yend = 0), linetype = "dashed")

```

## Other Principal Components

In general, if we have $p$ predictors, we can compute $p$ distinct principal components: $Z_1, Z_2, \dots , Z_p$.

\pause

The second principal component $Z_2$ is a linear combination of the centered variables that is 

  - uncorrelated with the first principal component
  - has the largest variance subject to this constraint.

\pause

For the case when $p =2$, the 2nd principal component corresponds to the line perpendicular to the line for the 1st principal component.

\pause

Generally, the $k$th principal component is obtained by finding a linear combination of centered variables that is uncorrelated with all previous principal components, and has the largest variance subject to this constraint.


## PCA Example

The second principal component


```{r out.width = "45%"}
phi21<-pca$rotation[2,1]
phi22<-pca$rotation[2,2]
ggplot(d, aes( x = Weight, y = Belly))+geom_point()+theme_bw()+labs(x =  "Weight", y = "Belly")+geom_abline(intercept = b0, slope = b1, color = "blue", size = 1)+geom_abline(slope = -1/b1, intercept = Belly_mean + 1/b1*Weight_mean, color = "red", linetype = "dashed", size = 1) + coord_fixed()
```

$$
Z_2 = 0.75\cdot (\textrm{Weight} - 24.1)  - 658 (\textrm{Belly} -  19.8)
$$

## Principal Comoponent Regression

The PCR approach to linear regression constructs the first $M$ principal components $Z_1, \dots, Z_M$ of a data set with $p$ predictors (so $M \leq p$), and then uses these as predictors in a linear regression model.

  \pause
  
  - Goal: Use a small number of predictors which explain most of the variability in the data set, as well as their relationship to the response.

\pause

In general, PCR tends to produce linear models with higher accuracy than models fit with the original predictors. 

```{r fig.height=3, fig.width=8, out.width="70%"}
library(gridExtra)
x <- seq(from = 0, to = 50, by = 5)
y <- seq(from = 0, to = 100, by = 10)
d<-data.frame(x,y)


 
g1<-ggplot( d, aes(x = x, y= y))+theme_bw()+labs(x = "Number of Components", y = "Error", title = "Original")
g2<-ggplot( d, aes(x = x, y= y))+theme_bw()+labs(x = "Number of Components", y = "Error", title = "PCA")
grid.arrange(g1, g2, ncol=2)
```

## Principal Component Regression in R

We can use the `pcr` function in the `pls` library to quickly perform PCR in R. 

\pause

The `Hitters` data set from the `ISLR` package contains `Salary` and 18 other predictors for 263 baseball players  

```{r}
set.seed(13)
library(ISLR)
library(rsample)
Hitters <- Hitters %>% drop_na()
Hitters_split <- initial_split(Hitters)
Hitters_train <- training(Hitters_split)
Hitters_test <- testing(Hitters_split)
```

\small

```{r echo = T}
set.seed(1)
library(pls)
my_pcr <- pcr( Salary ~ ., data = Hitters_train, scale = T, validation = "CV")
```


\pause

- Setting `scale = T` standardizes each predictor

- Setting `validation = "CV"` causes `pcr` to compute the 10-fold CV error for each value of $M$ (number of principal components used)


## PCR Results
\tiny

```{r echo = T}
summary(my_pcr)
```
\small

\pause

- Note: `pcr` reports RSE, so values need to be squared to get MSE. 


## Validation Plot
\small

```{r echo = T, out.width = "60%"}
validationplot(my_pcr, val.type = "RMSEP", legendpos = "top")
```

\pause

\normalsize

- Note: The smallest CV error occurs at $M = 17$ (which is close to the maximum number of predictors $p = 19$.)

\pause

- However, a relatively low CV error is also obtained at $M =5$, suggesting fewer components are sufficient


## Make Predictions

Finally, to actually implement the PCR model on training data, we use the `predict` function

\footnotesize

```{r echo = T}
pcr_preds5 <- predict(my_pcr, Hitters_test, ncomp = 5)
pcr_preds17 <- predict(my_pcr, Hitters_test, ncomp = 17)
pcr_preds19 <- predict(my_pcr, Hitters_test, ncomp = 19)

results <- data.frame(obs = Hitters_test$Salary, pcr_preds5, pcr_preds17,pcr_preds19) %>% 
  pivot_longer(!obs, names_to = "model", values_to = "preds")

library(yardstick)
results %>% group_by(model) %>% rmse(truth = obs, estimate = preds ) %>% arrange(.estimate)
```

 
