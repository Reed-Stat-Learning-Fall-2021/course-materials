---
title: "Logistic Regression"
author: "Nate Wells"
date: "October 27th, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	cache = F,
	fig.width = 6,
	fig.height = 4,
	warning = FALSE,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ISLR)
library(ggforce) 
```

## Outline

In today's class, we will...

- Discuss further theory of logistic regression
 
- Implement logistic regression in R


# Logistic Regression Theory

## Summary

- In a classification problem, we are interested a categorical response variable $Y$.

\pause

- We might be interested in **predicting** the class for $Y$ based on observations, or we might be interested in **inferring** the relationships between $Y$ and predictors.

\pause

- Ideally, we would like to estimate the conditional probability of $Y$ given $X$
$$
P(Y = A_j| X)
$$

\pause

- For binary response $Y$, we can use logistic regression, which assumes the log-odds of $Y=1$ is linear:
$$
\ln \frac{ P(Y = 1|X)}{1 - P(Y = 1|X)} = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
$$

\pause

- This implies the conditional probability is logistic:
$$
P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}
$$

\pause

- To classify, we assign a test observation the value $1$ if
$$
P(Y = 1|X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}} \geq 0.5
$$

## Effect of Coefficients in Logistic Model

- Consider a logistic regression model for a binary categorical variable $Y$ based on a single predictor $X$. \small
$$
\ln \frac{p(X)}{1 - p(X)} = \beta_0 + \beta_1 X \qquad p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
$$

```{r}
t <- seq(-10, 10, length = 100)
logistic <- function(x,a,b){exp(a + b*x)/(1 + exp(a + b*x))}
A<-logistic(t,0,1)
B<-logistic(t,0,.5)
C<-logistic(t,0,4)
D<-logistic(t,0,-1)
E<-logistic(t,0,-4)

d <- data.frame(t, A, B, C,D,E) %>% pivot_longer(!t, names_to = "slope") %>% 
  mutate(slope = case_when(
    slope == "A" ~ "1",
    slope == "B" ~ "0.5",
    slope == "C" ~ "4",
    slope == "D" ~ "-1",
    slope == "E" ~ "-4",
  )) %>% 
  mutate(slope = fct_relevel(slope, "-4"))

ggplot(d, aes(x =t, y = value, color = slope))+geom_line()+theme_bw()+labs(title = "Effect of Slope, with constant intercept of 0")
```




## Effect of Coefficients in Logistic Model

- Consider a logistic regression model for a binary categorical variable $Y$ based on a single predictor $X$. \small
$$
\ln \frac{p(X)}{1 - p(X)} = \beta_0 + \beta_1 X \qquad p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
$$

```{r}
A<-logistic(t,0,1)
B<-logistic(t,1, 1)
C<-logistic(t,4,1)
D<-logistic(t,-4,1)
E<-logistic(t,-1,1)

d <- data.frame(t, A, B, C,D,E) %>% pivot_longer(!t, names_to = "intercept") %>% 
  mutate(intercept = case_when(
    intercept == "A" ~ "0",
    intercept == "B" ~ "1",
    intercept == "C" ~ "4",
    intercept == "D" ~ "-4",
    intercept == "E" ~ "-1"
  )) %>% 
  mutate(intercept = fct_relevel(intercept, "-4"))

ggplot(d, aes(x =t, y = value, color = intercept))+geom_line()+theme_bw()+labs(title = "Effect of Intercept, with constant slope of 1")
```


## Effect of Coefficients in Logistic Model

- Consider a logistic regression model for a binary categorical variable $Y$ based on a single predictor $X$. \small
$$
\ln \frac{p(X)}{1 - p(X)} = \beta_0 + \beta_1 X \qquad p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
$$

```{r}
A<-logistic(t,0,-1)
B<-logistic(t,1, -1)
C<-logistic(t,4,-1)
D<-logistic(t,-4,-1)
E<-logistic(t,-1,-1)

d <- data.frame(t, A, B, C,D,E) %>% pivot_longer(!t, names_to = "intercept") %>% 
  mutate(intercept = case_when(
    intercept == "A" ~ "0",
    intercept == "B" ~ "1",
    intercept == "C" ~ "4",
    intercept == "D" ~ "-4",
    intercept == "E" ~ "-1"
  )) %>% 
  mutate(intercept = fct_relevel(intercept, "-4"))

ggplot(d, aes(x =t, y = value, color = intercept))+geom_line()+theme_bw()+labs(title = "Effect of Intercept, with constant slope of -1")
```


## Regression Coefficient Estimates

- Assume that the log-odds of $Y = 1$ is indeed linear in $X_1, \dots , X_p$, so that
$$
\ln \frac{p(X)}{1 - p(X)} = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
$$

  \pause
  
  - We need to estimate the parameters $\beta_0, \beta_1, \dots, \beta_p$ based on training data.

\pause

- We could use the Method of Least Squares, as we did with Linear Regression.

  \pause
  
  - But there isn't a closed-from solution as in Linear Regression
  
  \pause
  
  - And in practice, residuals tend not to be approximately Normally distributed
  
\pause

- Instead, we use the method of **Maximum Likelihood** (ML)

## The Method of Maximum Likelihood

- Under ML, we compare all possible models and select the one for which the observed data had highest probability of occurring 

\pause

- Suppose we have $k$ observations with $y = 1$ and $n-k$ with $y = 0$.

  \pause
  
  - Assume we've relabeled indices so the first $k$ observations have $y = 1$

  \pause
  
  \normalsize 

  - As before, we assume \small
$$
p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}
$$

\pause

- Then the probability of the observed data is \small
$$
\ell(\beta_0, \beta_1, \dots, \beta_p) = \prod_{i = 1}^k p(x_i) \prod_{j = k+1}^n (1 - p(x_{j}))
$$

  \pause
  \normalsize
  -  View $\ell$ as a function of parameters $\beta_0, \dots, \beta_p$ for **fixed** observations $x_1, \dots, x_n$.


\pause
  
- The goal is to choose $\hat{\beta}_0, \hat \beta_1, \dots , \hat{\beta}_p$ so as to maximize $\ell$

  \pause

  - How? (Calculus or numeric methods, or R!)
  
 



# Logistic Regression Practice


## The Unsinkable Example

The `Titanic` data set contains information on passengers of the *Titanic*

\tiny

```{r}
Titanic<-read_csv("data/titanic.csv")
glimpse(Titanic)
```
\normalsize

- Goal: Determine relationship between survival, sex, and age.

\pause

- Is this primarily an inference or prediction task?

  \pause

  - Can it be neither?



## Data Analysis

\tiny

```{r echo = T}
library(skimr)
Titanic %>% select(age, sex, survived) %>% summary()
Titanic %>% count(sex)
Titanic %>% count(survived)
```
\normalsize

- What are some concerns we may have about variables `sex`, `age` and `survival`?

\pause

\tiny

```{r echo = T}
library(tidyr)
Titanic1<-Titanic %>% drop_na(age)
```


## Children first?

- Who survived the Titanic?
\footnotesize

```{r echo =F}
Titanic1 %>% ggplot( aes( x = age, y = survived))+ 
  geom_jitter(height = .01, alpha = .25)+theme_bw()
```
## Women First?

- Who survived the Titanic?
\footnotesize

```{r echo =F}
Titanic1 %>%  mutate(survived = as.factor(survived)) %>% 
  ggplot( aes( x = sex, fill = survived))+ 
  geom_bar(position = "fill")+theme_bw()
```
## Women and Children First?

\footnotesize

```{r echo =T}
Titanic1 %>% ggplot( aes( x = age, y = survived, color = sex))+ 
  geom_jitter(height = .01, alpha = .5)+theme_bw()
```


## Logistic Model 1

 

\footnotesize

```{r echo =T, out.width = "60%"}
Titanic1 %>% ggplot( aes( x = age, y = survived ))+ 
  geom_jitter(height = .01, alpha = .5)+theme_bw()+
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F) 
```

$$
p(X) = \frac{e^{0.117 - 0.01X}}{1 + e^{0.117 - 0.01X}}
$$

## VS Linear Model

 

\footnotesize

```{r echo =T, out.width = "60%"}
Titanic1 %>% ggplot( aes( x = age, y = survived ))+ 
  geom_jitter(height = .01, alpha = .5)+theme_bw()+
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F,size = 2,linetype = "dashed") +
  geom_smooth(method = "lm", se = F, color  = "red")
```

$$
p(X) = 0.528 - 0.003X
$$



## Logistic Model 2:

 

\footnotesize

```{r echo =T}
library(moderndive)
Titanic1 %>% ggplot( aes( x = age, y = survived, color = sex ))+ 
  geom_jitter(height = .01, alpha = .5)+theme_bw()+
  geom_parallel_slopes(method = "glm", method.args = list(family = "binomial"), se = F)+
  labs(title = "Logistic Regression, survival ~ age + sex")
```

## Logistic Model 3:

 

\footnotesize

```{r echo =T}
library(moderndive)
Titanic1 %>% ggplot( aes( x = age, y = survived, color = sex ))+ 
  geom_jitter(height = .01, alpha = .5)+theme_bw()+
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F)+
  labs(title = "Logistic Regression, survival ~ age*sex")
```

## R code for Logistic Models

\columnsbegin
\column{.5\textwidth}

\tiny

```{r echo = T}
simple_logreg <- glm(survived ~ age, data = Titanic1, family = "binomial")
summary(simple_logreg)
```

\column{.45\textwidth}

\small 

\pause

- The logistic model is $$\ln \frac{p(\textrm{Age})}{1 - p(\textrm{Age})} = 0.11 - 0.01  \cdot \textrm{Age}$$

\pause

- Since $$e^{-0.011} = 0.989 = 1 - 0.011$$ increasing age by 1 year decreases survival probability by $1.1\%$ **of the current probability.**


\columnsend


## R code for Logistic Models

\columnsbegin
\column{.5\textwidth}

\tiny

```{r echo = T}
simple_logreg <- glm(survived ~ age, data = Titanic1, family = "binomial")
summary(simple_logreg)
```

\column{.45\textwidth}

\small 

- Where is RSE? $R^2$? $F$-stat?

\pause

- Logistic regression is from the family of *generalized linear models*

  - GLiMs use *deviance* as metric of model fit.
  
  - Null deviance measures how well the null model (only intercept) predicts the data
  
  - Residual deviance measures how well the fitted model predicts the data
  
\pause

- Fisher Scoring Iterations indicates the number of loops of numeric optimization algorithm

\columnsend



## R code for Multiple Logistic Models

- Suppose we fit a logistic model for `survived ~ age + sex`:

\columnsbegin
\column{.5\textwidth}

\tiny

```{r echo = T}
logreg <- glm(survived ~ age + sex, data = Titanic1, family = "binomial")
summary(logreg) 
```

\column{.45\textwidth}

\small 

\pause
- What is the formula for the logistic model?
 

\pause

- What is the survival probability for a male child of age $5$? A female child of age 5?

\pause

- What effect does being male have on survival probability?

\columnsend


## R code for Multiple Logistic Models

- Suppose we fit a logistic model for `survived ~ age * sex`:

\columnsbegin
\column{.5\textwidth}

\tiny

```{r echo = T}
logreg2 <- glm(survived ~ age * sex, data = Titanic1, family = "binomial")
summary(logreg2) 
```

\column{.45\textwidth}

\small 

\pause
- What is the formula for the logistic model?

\pause

- What is the survival probability for a male child of age $5$? A female child of age 5?

\pause

- What effect does being male have on survival probability?

\columnsend


# Classification

## Classification using Logistic Regression

Develop a classification scheme based on the linear regression model.

\pause

$$
\hat{Y} = \begin{cases} 1,& \textrm{ if } p(X) \geq 1 - p(X),\\ 0,& \textrm{ otherwise.}
\end{cases}
$$

\pause

$$
\hat{Y} = \begin{cases} 1,& \textrm{ if odds } \geq 1,\\ 0,& \textrm{ if odds } < 1
\end{cases}
$$

\pause


$$
\hat{Y} = \begin{cases} 1,& \textrm{ if log odds } \geq 0,\\ 0,& \textrm{ if log odds } < 0
\end{cases}
$$

## Prediction and Classification in R

Suppose we have $10$ hypothetical passengers with the following age/sex combinations:

\small

```{r }
set.seed(1)
passengers<- data.frame(age = seq(10, 46, length.out = 10), sex = sample(c("male", "female"), size = 10, replace = T))
```

 

```{r echo = T}
passengers
```

## Prediction and Classification in R

\normalsize

What are their survival log odds?

\footnotesize

```{r echo = T, out.width = "100%"}
odds<- predict(logreg2, passengers)
odds
```
\pause

\normalsize

Survival probabilities?

\footnotesize

```{r echo = T}
probs <- predict(logreg2, passengers, type = "response")
probs
```

\normalsize

\pause

Classification?

\footnotesize

```{r echo = T}
ifelse(probs >= .5, 1, 0)
```

## Confusion Matrix

How well does our model do on training data? We'll use several functions from the `yardstick` package.

\pause

- First, we create data frame comparing observed and predicted classes:

\footnotesize

\pause

```{r echo  = T}
probs<-predict(logreg2, Titanic1, type = "response")
preds<-as.factor( ifelse(probs >=.5, 1, 0))
obs <- as.factor(Titanic1$survived)
results <- data.frame(obs, preds)
```

\pause

\normalsize

- And then create a **confusion matrix** using `conf_mat` from `yardstick`

\footnotesize

```{r echo = T}
library(yardstick)
conf_mat(results, truth  = obs, estimate = preds)
```


## Error Measures


- The overall error rate is the proportion of incorrect classifications:
$$
\textrm{error rate} = \frac{1}{n} \sum_{i=1}^n I(y_i \neq \hat{y}_i)
$$
\pause

- In `yardstick`, the `accuracy` function returns the proportion of correct classifications:

\footnotesize

```{r echo = T}
accuracy(results, truth = obs, estimate = preds)
```
\normalsize

- Accuracy is the sum of the diagonal elements in the confusion matrix divided by the total number of observations.


\pause

\normalsize

- To obtain the error rate, we pull the accuracy estimate and subtract from 1:

\footnotesize

```{r echo = T}
acc <- accuracy(results, truth = obs, estimate = preds) %>% pull(.estimate)
1 - acc
```

