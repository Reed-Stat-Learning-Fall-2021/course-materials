---
title: "K-Nearest Neighbor"
author: "Nate Wells"
date: "September 10th, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	cache = TRUE,
	fig.width = 6,
	fig.height = 4,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
library(ggthemes)
library(moderndive)
library(ISLR)
```

## Outline

In today's class, we will...

- Discuss the Bayes Classifier

- Implement KNN as estimate for Bayes Classifier


 
# The Bayes Classifier

## The Task

Suppose $Y$ is categorical response variable with several levels $A_1, \dots, A_k$. 

Goal: Build a model $f$ to classify an observation into levels $A$ or $B$ based on the values of several predictors $X_1, X_2, \dots , X_p$ (quantitative or categorical)

$$
Y = f(X_1, X_2, \dots , X_p) + \epsilon \qquad \textrm{where }f, \epsilon \textrm{ take values in } \{A_1, \dots , A_k\}
$$

\pause

How do we measure accuracy of our model?

\pause

- Training data: Compute error rate on observations in training data:
$$
\textrm{Training Error} = \frac{1}{n}\sum_{i =1}^n I(y_i \neq \hat{y}_i)
$$
where $I(y_i \neq \hat{y}_i)$ is the indicator variable that equals $1$ if $y_i \neq \hat{y}_i$ and $0$ otherwise.

\pause

- Test data: Compute average proportion of errors on test data
$$
\textrm{Test Error} = \textrm{Avg. } I(y_0 \neq \hat{y}_0)
$$
where $\hat{y}_0$ is the predicted class for a test observation with predictor $x_0$.

## The Best Possible Model

In general, the value of a response $Y$ may depend on more than just the values of the predictors $X_1, \dots, X_p$ in a model.

  \pause
  
  - That is, given the value of predictors $x_0$, the value of the response $y_0$ is random.
  
\pause

We can show that the model which minimizes test error is
$$
f(x_0) = \mathrm{argmax}_j P(Y = A_j \, | \, X = x_0 )
$$
  \pause
  
  - A proof can be found on p. 18-22 of Elements of Statistical Learning (req. Math 391)
  
\pause

- In practice, we cannot build this optimal model, since we don't know $P(Y = A_j \, |\, X = x_0)$

## Simulation

Suppose $Y$ takes two values $A$ and $B$, and $X_1$ and $X_2$ are predictors taking values in $[0,1]$.

\pause

Moreover, suppose the probability $Y = A$ given $X_1 = x_1$ and $X_2 = x_2$ is $(x_1^2 + x_2^2)/2$

\pause

```{r eval = F, echo =T}
set.seed(1)
n<-200
x1<-runif(n, 0,1 )
x2<-runif(n, 0,1)
p<-(x1^2 + x2^2)/2 
```

\pause

Then
$$
f(x_0) = \mathrm{argmax}_j P(Y = A_j \, | \, X = x_0 ) = \begin{cases}A,& \textrm{ if } x_1^2 +x_2^2 \geq 1 \\ B,& \textrm{ if } x_1^2 +x_2^2 <1   \end{cases}
$$

## Plot 1

```{r cache=T}
set.seed(1)
n<-200
x1<-runif(n, 0,1 )
x2<-runif(n, 0,1)
p<-(x1^2 + x2^2)/2 
Y<-c()
for (i in 1:n){
  Y<-c(Y, sample(c("A","B"), 1, prob = c(p[i], 1 - p[i])))
}
d<-data.frame(x1,x2,Y)


ggplot(d, aes(x = x1, y =x2, color = Y))+geom_point()
```

## Plot 2

```{r cache=T}
set.seed(1)
 

X<-seq(0,1,length.out = 200)
Z<-sqrt(1 - X^2)
d2<-data.frame(X,Z)
  
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "red", size = 1)
```

## Expected Error Rate

In general, using the Bayes Classifier produces an expected error rate of
$$
1 - \mathrm{Avg.} \left( \max_j \mathrm{P}(Y = A_j \, | \, X = x_0) \right)
$$

\pause

For our simulation, this gives an error of $1/3$.

\pause

  - Can verify using multivariate calculus or by sampling a large number of times.
  
\pause

This is the theoretical lower bound on average error for a classification problem.

# K-Nearest Neighbors

## From Bayes Classifier to KNN

In theory, the Bayes Classifier is our best model for classification.

\pause

- In practice, we don't know the conditional probability of $Y$ given $X$, and so cannot build a Bayes Classifier model.

\pause

- But given sufficient data, we can *estimate* the conditional probabilities (assuming they are generated by a continuous function).

\pause

Given a positive integer $K$ and a test observation $x_0$, let $N_0$ denote the $K$ nearest training observations to $x_0$. Then
$$
P(Y = A_j \, | \, X = x_0 ) \approx \frac{1}{K} \sum_{i \in N_0} I(y_i = A_j)
$$
\pause

- Our model is therefore $f(x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = A_j)$.

## Simulation

Classify $x_0$ for a variety of $K$

```{r cache =T}
set.seed(22)
a<-runif(10, 0, 1)
b<-runif(10, 0, 1)
Y<-sample(c("A","B"), replace = T, size = 10)
d3<-data.frame(a,b,Y)

ggplot(d3, aes(x = a, y = b, color = Y))+geom_point()+annotate(geom = "point", x = .5, y = .5, color = "purple") + annotate(geom = "text", x = .53, y = .52, label = "x0")+labs(x = "X1", y = "X2" )
```
## Simulation 

Sketch the classification boundaries for $K = 3$. What happens for $K = 1$? As $K$ gets larger?

```{r cache=T}
ggplot(d3, aes(x = a, y = b, color = Y))+geom_point()
```

## Error Rates

Sketch the graph of KNN error rates as function of $K^{-1}$

```{r}
K<-seq(0,1, length.out = 20)
E<-seq(0, .25, length.out = 20)
d4<-data.frame(K,E)

ggplot(d4, aes(x = K, y =E))+labs(x = "1/K", y = "Error Rate")
```

## Extra Practice

Use the first part of the .Rmd file on the course website to generate 5 random points and form classification boundaries for $K =1$ and $K=2$ KNN.

Then use the second part of the .Rmd file to classify 5 randomly generated points.