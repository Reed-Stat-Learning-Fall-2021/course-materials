---
title: "K-Nearest Neighbor"
author: "Nate Wells"
date: "September 10th, 2021"
institute: "Math 243: Stat Learning"
fontsize: 9pt
output: 
  beamer_presentation:
    includes:
      in_header: preamble.tex    
graphics: yes    
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	cache = T,
	fig.width = 6,
	fig.height = 4,
	out.width = "70%"
)
library(tidyverse)
library(knitr)
library(ggthemes)
library(moderndive)
library(ISLR)
```

## Outline

In today's class, we will...

- Discuss the Bayes Classifier

- Implement KNN as estimate for Bayes Classifier


 
# The Bayes Classifier

## The Task

Suppose $Y$ is categorical response variable with several levels $A_1, \dots, A_k$, and that $X_1, \dots, X_p$ are predictors (either categorical or quantitative).

  - Assume that the level of $Y$ is not completely determined by the values of $X_1, \dots, X_p$.

\pause

Goal: Build a model $\hat g(X_1, \dots, X_p)$ that takes values in $\{A_1, \dots, A_p\}$ that can be used to predict the class of $Y$ based on $X_1, \dots, X_p$. 


\pause

 - How do we measure accuracy of our model? - Why not MSE?
  
\pause

- Training data: Compute error rate on observations in training data:
$$
\textrm{Training Error} = \frac{1}{n}\sum_{i =1}^n I\big(y_i \neq \hat{g}(x_i)\big)
$$
where $I\big(y_i \neq \hat{g}(x_i)\big)$ equals $1$ if $y_i \neq \hat{g}(x_i)$ and $0$ otherwise.

\pause

- Test data: Compute average proportion of errors on test data
$$
\textrm{Test Error} = \textrm{Avg. } I\big(y_i \neq \hat{g}(x_0)\big)
$$
with the average taken across many test observations $x_0$.

## The Best Possible Model

In general, the value of a response $Y$ may depend on more than just the values of the predictors $X_1, \dots, X_p$ in a model.

  \pause
  
  - That is, given the value of predictors $x_0$, the value of the response $y_0$ is random.
  
\pause

The model (called the **Bayes Classifier**) which minimizes test error is
$$
g(x_0) = \mathrm{argmax}_{A_j} P(Y = A_j \, | \, X = x_0 )
$$
  \pause
  
  - This model assigns $Y$ to the most likely class, given the value of $x_0$.
  
  - A proof can be found on p. 18-22 of Elements of Statistical Learning (uses tools from adv. probability)
  
\pause

- In practice, we cannot build this optimal model, since we don't know know the formula for $P(Y = A_j \, |\, X = x_0)$

## Simulation

- Suppose $Y$ takes values $A$ or $B$, and $X_1$ and $X_2$ are predictors taking values in $[0,1]$.

\pause

- Additionally, suppose that if $X_1 = x_1$ and $X_2 = x_2$, then $Y = A$ with probability $$p = (x_1^2 + x_2^2)/2$$

\pause

\columnsbegin
\column{.45 \textwidth}
```{r}
big_grid <-expand.grid(x1 = seq(from = 0, to = 1, length.out = 60), x2 =  seq(from = 0, to = 1, length.out = 60) ) %>% mutate(p = (x1^2 + x2^2)/2)
```

```{r out.width = "120%"}
ggplot(big_grid, aes(x = x1, y = x2, fill = p))+
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red', midpoint = .5)+
  geom_tile()+coord_fixed()+theme_bw()+labs(title = "Probability of Y = A")
```

\column{.45 \textwidth}

\pause

- What is the Bayes Classifier $g$?

\pause
\begin{align*}
g(x_0) =& \mathrm{argmax}_{A_j} P(Y = A_j \, | \, X = x_0 ) \\ =& \begin{cases}A,& \textrm{ if } x_1^2 +x_2^2 \geq 1 \\ B,& \textrm{ if } x_1^2 +x_2^2 <1   \end{cases}
\end{align*}


\columnsend




## Simulate Data

Let's simualte 200 data points from this model.

```{r cache = T}
set.seed(10)
n<-200
x1<-runif(n, 0,1 )
x2<-runif(n, 0,1)
p<-(x1^2 + x2^2)/2 
Y<-c()
for (i in 1:n){
  Y<-c(Y, sample(c("A","B"), 1, prob = c(p[i], 1 - p[i])))
}
d<-data.frame(x1,x2,Y)


ggplot(d, aes(x = x1, y =x2, color = Y))+geom_point()+coord_fixed()+theme_bw()
```

## The Bayes Classifier

The purple arc represents the Bayes Classifier boundary

```{r cache=T}
X<-seq(0,1,length.out = 200)
Z<-sqrt(1 - X^2)
d2<-data.frame(X,Z)
  
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1)+coord_fixed()+theme_bw()
```

## Expected Error Rate

In general, using the Bayes Classifier produces an expected error rate of
$$
1 - \mathrm{Avg.} \left( \max_j \mathrm{P}(Y = A_j \, | \, X = x_0) \right)
$$

\pause

- For our simulation, this gives an error of $\frac{2}{3} - \frac{\pi}{8} \approx 0.274$.

  \pause

  - Can verify using multivariate calculus or by sampling a large number of times.
  
\pause

- This is the theoretical lower bound on average test error for this classification problem.

  \pause
  
  - This is analogous to the irreducible error in regression problems
  
  

# K-Nearest Neighbors

## From Bayes Classifier to KNN

In theory, the Bayes Classifier is our best model for classification.

\pause

- In practice, we don't know the conditional probability of $Y$ given $X$, and so cannot build a Bayes Classifier model.

\pause

- But given sufficient data, we can *estimate* the conditional probabilities (assuming they are generated by a continuous function).

\pause

Given a positive integer $K$ and a test observation $x_0$, let $N_0$ denote the $K$ nearest training observations to $x_0$. Then
$$
P(Y = A_j \, | \, X = x_0 ) \approx \frac{1}{K} \sum_{i \in N_0} I(y_i = A_j)
$$
\pause

- Our model for $P$ is therefore $\hat P_j(x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = A_j)$.

\pause

- And our classifier model is $\hat g (x_0) = \mathrm{argmax}_{A_j} \hat P_j(x_0)$


## Classify Points

Classify $x_0$ for $K = 1, 2, 3, 5, 10, 200$.

```{r cache =T}
ggplot(d, aes(x = x1, y =x2, color = Y))+geom_point()+coord_fixed()+theme_bw()+annotate(geom = "point", x = .4, y = .83, color = "purple", size = 2) + annotate(geom = "text", x = .35, y = .85, label = "x0")+labs(x = "X1", y = "X2" )

#ggplot(d3, aes(x = a, y = b, color = Y))+geom_point()+annotate(geom = "point", x = .5, y = .5, color = "purple") + annotate(geom = "text", x = .53, y = .52, label = "x0")+labs(x = "X1", y = "X2" )
```


## Classification Boundaries 

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+coord_fixed()+theme_bw()+labs(title = "Training Data and Bayes Classifier")
```

```{r}
library(class)
test <- expand.grid(x1 = seq(from = 0, to = 1, length.out = 60), x2 =  seq(from = 0, to = 1, length.out = 60) )

k1 <- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =1
))

k2<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =2
))

k5<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =5
))

k10<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =10
))

k20<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =20
))

k50<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =50
))

k100<- data.frame(test,Y = knn(
 d %>% select(x1,x2), test, d$Y, k =100
))
```


## k=1 

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k1, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 1")
```


## k=2

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k2, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 2")
```


## k=5

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k5, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 5")
```

## k=10

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k10, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 10")
```


## k=20

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k20, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 20")
```

## k=50

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k50, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 50")
```

## k=100

Here are the classification boundaries for a variety of values of $K$.  

```{r cache=T}
ggplot()+geom_point(data = d, aes(x = x1, y =x2, color = Y))+geom_line(data = d2, aes(x = X, y = Z), color = "purple", size = 1, alpha = .5)+
  geom_point(data = k100, aes(x = x1, y = x2, color = Y), alpha = .35, size = 1)+
  coord_fixed()+theme_bw()+labs(title = "k = 100")
```


## Error Rates

The graph below shows error rates for the training set, as well as a test set of 100 points.

```{r}
k <- seq(from = 1, to = 101, by = 4)
train_error <- c()

for (i in k){
  preds <- knn(d %>% select(x1,x2), d %>% select(x1,x2), d$Y, k =i)
  t <- table(preds, d$Y)
  new_error <- (sum(t)- sum(diag(t)))/sum(t)
  train_error <- c(train_error, new_error)
}
```

```{r}
set.seed(252)
n<-100
x1<-runif(n, 0,1 )
x2<-runif(n, 0,1)
p<-(x1^2 + x2^2)/2 
Y<-c()
for (i in 1:n){
  Y<-c(Y, sample(c("A","B"), 1, prob = c(p[i], 1 - p[i])))
}
new_d<-data.frame(x1,x2,Y)
```

```{r}
test_error <- c()

for (i in k){
  preds <- knn(d %>% select(x1,x2), new_d %>% select(x1,x2), d$Y, k =i)
  t <- table(preds, new_d$Y)
  new_error <- (sum(t)- sum(diag(t)))/sum(t)
  test_error <- c(test_error, new_error)
}  
```

```{r}
data.frame(k, train_error, test_error) %>% 
pivot_longer(!k, names_to = "error_type", values_to = "error") %>% 
ggplot( aes(x = k, y = error, color =error_type))+
  geom_smooth(se = F, size = 1)+
  geom_point()+
  theme_bw()+
  scale_x_reverse()+
  labs(title = "Error Rates for KNN")
```


## Extra Practice

1. Use the first part of the .Rmd file on the course website to generate 4 random points and form classification boundaries for $K =1$ and $K=2$ KNN.

2. Then use the second part of the .Rmd file to classify 5 randomly generated points.